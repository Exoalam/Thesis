{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "620298b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ভষ</w>', 'ি</w>', 'বল</w>', 'ভলোবস', 'মি</w>'}\n"
     ]
    }
   ],
   "source": [
    "# ... [same functions as before: get_stats and merge_vocab]\n",
    "\n",
    "def get_stats(vocab):\n",
    "    pairs = {}\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols) - 1):\n",
    "            pair = (symbols[i], symbols[i + 1])\n",
    "            if pair in pairs:\n",
    "                pairs[pair] += freq\n",
    "            else:\n",
    "                pairs[pair] = freq\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, v_in):\n",
    "    v_out = {}\n",
    "    bigram = ' '.join(pair)\n",
    "    replacement = ''.join(pair)\n",
    "    for word in v_in:\n",
    "        w_out = word.replace(bigram, replacement)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    return v_out\n",
    "def byte_pair_encode_bangla(text, num_merges):\n",
    "    # Initialize vocabulary with individual Bengali characters\n",
    "    vocab = {}\n",
    "    for word in text.split():\n",
    "        word = ' '.join(word) + ' </w>'  # Adding end of word token\n",
    "        if word in vocab:\n",
    "            vocab[word] += 1\n",
    "        else:\n",
    "            vocab[word] = 1\n",
    "    \n",
    "    for i in range(num_merges):\n",
    "        pairs = get_stats(vocab)\n",
    "        if not pairs:\n",
    "            break\n",
    "        best = max(pairs, key=pairs.get)\n",
    "        vocab = merge_vocab(best, vocab)\n",
    "    \n",
    "    # Extracting the tokens from the vocabulary\n",
    "    bpe_tokens = set()\n",
    "    for word in vocab.keys():\n",
    "        bpe_tokens.update(word.split())\n",
    "    \n",
    "    return bpe_tokens\n",
    "\n",
    "bangla_text = \"মি বল ভষ ভলোবসি\"  # This translates to \"I love the Bengali language\"\n",
    "tokens = byte_pair_encode_bangla(bangla_text, 10)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f64ebde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁আমি', '▁ভাত', '▁খাই', '।', '▁সে', '▁বাজারে', '▁যায়', '।']\n",
      "[914, 5265, 24224, 3, 124, 2244, 41, 3]\n",
      "আমি ভাত খাই। সে বাজারে যায়।\n"
     ]
    }
   ],
   "source": [
    "from bnlp import SentencepieceTokenizer\n",
    "\n",
    "bsp = SentencepieceTokenizer()\n",
    "input_text = \"আমি ভাত খাই। সে বাজারে যায়।\"\n",
    "tokens = bsp.tokenize(input_text)\n",
    "print(tokens)\n",
    "text2id = bsp.text2id(input_text)\n",
    "print(text2id)\n",
    "id2text = bsp.id2text(text2id)\n",
    "print(id2text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2609b1ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "014a0960c39a46bbbdc5326af351aa26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23727fe2f4f5465ca16dcc89367f8f7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d9bcf87568a4c07b58bb15ecc45a1af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "541e83e8c53e40ea9d1cf11f46d7f4d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db4c163e379945bd9b953b43d0e9cb1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2729, 0.7271]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load pre-trained model tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Encode text\n",
    "input_text = \"Hello, how are you?\"\n",
    "tokens = tokenizer(input_text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Load pre-trained model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Model output\n",
    "outputs = model(**tokens)\n",
    "\n",
    "# Logits (classification scores)\n",
    "logits = outputs.logits\n",
    "\n",
    "# Convert logits to probabilities\n",
    "probabilities = F.softmax(logits, dim=1)\n",
    "\n",
    "# Print the classification results\n",
    "print(probabilities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30ec7f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
