{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "620298b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ি', 'য', 'র', '</w>', 'ব', 'ে', 'খ', 'ম', 'স', 'া', 'ত', 'জ', 'য়', 'আ', '।', 'ভ', 'ই', '।</w>', 'ে</w>', 'আম', 'আমি', 'আমি</w>', 'ভা', 'ভাত', 'ভাত</w>', 'খা']\n"
     ]
    }
   ],
   "source": [
    "def get_stats(vocab):\n",
    "    pairs = {}\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols) - 1):\n",
    "            pair = (symbols[i], symbols[i + 1])\n",
    "            if pair in pairs:\n",
    "                pairs[pair] += freq\n",
    "            else:\n",
    "                pairs[pair] = freq\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, v_in):\n",
    "    v_out = {}\n",
    "    bigram = ' '.join(pair)\n",
    "    replacement = ''.join(pair)\n",
    "    for word in v_in:\n",
    "        w_out = word.replace(bigram, replacement)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    return v_out\n",
    "    \n",
    "def byte_pair_encode_bangla(text, num_merges):\n",
    "    vocab = {}\n",
    "    for word in text.split():\n",
    "        word = ' '.join(word) + ' </w>'  \n",
    "        if word in vocab:\n",
    "            vocab[word] += 1\n",
    "        else:\n",
    "            vocab[word] = 1\n",
    "    \n",
    "    for i in range(num_merges):\n",
    "        pairs = get_stats(vocab)\n",
    "        if not pairs:\n",
    "            break\n",
    "        best = max(pairs, key=pairs.get)\n",
    "        vocab = merge_vocab(best, vocab)\n",
    "    \n",
    "    bpe_tokens = set()\n",
    "    for word in vocab.keys():\n",
    "        bpe_tokens.update(word.split())\n",
    "    \n",
    "    return bpe_tokens\n",
    "\n",
    "bangla_text = \"আমি ভাত খাই। সে বাজারে যায়।\" \n",
    "tokens = []\n",
    "for i in range(10):\n",
    "    token = byte_pair_encode_bangla(bangla_text, i)\n",
    "    for x in token:\n",
    "        if x not in tokens:\n",
    "            tokens.append(x)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f64ebde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁আমি', '▁ভাত', '▁খাই', '।', '▁সে', '▁বাজারে', '▁যায়', '।']\n",
      "[914, 5265, 24224, 3, 124, 2244, 41, 3]\n",
      "আমি ভাত খাই। সে বাজারে যায়।\n"
     ]
    }
   ],
   "source": [
    "from bnlp import SentencepieceTokenizer\n",
    "\n",
    "bsp = SentencepieceTokenizer()\n",
    "input_text = \"আমি ভাত খাই। সে বাজারে যায়।\"\n",
    "tokens = bsp.tokenize(input_text)\n",
    "print(tokens)\n",
    "text2id = bsp.text2id(input_text)\n",
    "print(text2id)\n",
    "id2text = bsp.id2text(text2id)\n",
    "print(id2text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2609b1ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "213fca009c884f71aba6216c68f17219",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/491 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbeadeca5c9f4844bfdd9c8af586819a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/2.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tokenizer is NOT compatible with GPT2.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer,AutoTokenizer, AutoModel\n",
    "\n",
    "def is_tokenizer_compatible_with_gpt2(tokenizer):\n",
    "    # Check if the tokenizer is an instance of GPT2Tokenizer\n",
    "    return isinstance(tokenizer, GPT2Tokenizer)\n",
    "\n",
    "# Instantiate a GPT2 tokenizer\n",
    "gpt2_tokenizer = AutoTokenizer.from_pretrained(\"sagorsarker/bangla-bert-base\")\n",
    "\n",
    "# Check if the tokenizer is compatible with GPT2\n",
    "if is_tokenizer_compatible_with_gpt2(gpt2_tokenizer):\n",
    "    print(\"The tokenizer is compatible with GPT2.\")\n",
    "else:\n",
    "    print(\"The tokenizer is NOT compatible with GPT2.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30ec7f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
